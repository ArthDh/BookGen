{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_GOT.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "AdAVuHU294w5",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "721d31e9-70a8-440f-87ed-ba0cc6d380e0",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1531189337530,
          "user_tz": -330,
          "elapsed": 15918,
          "user": {
            "displayName": "Arth Dh",
            "photoUrl": "//lh5.googleusercontent.com/-k3dUymHUDVw/AAAAAAAAAAI/AAAAAAAANZs/XNCIjM0QmZU/s50-c-k-no/photo.jpg",
            "userId": "110320452319374550954"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\r\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tul940nA-CFb",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FrUCGlvp-Gb0",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "126c8b63-01ba-43da-ddce-3ed5cbaf75b8",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1531189357229,
          "user_tz": -330,
          "elapsed": 7437,
          "user": {
            "displayName": "Arth Dh",
            "photoUrl": "//lh5.googleusercontent.com/-k3dUymHUDVw/AAAAAAAAAAI/AAAAAAAANZs/XNCIjM0QmZU/s50-c-k-no/photo.jpg",
            "userId": "110320452319374550954"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pBernJpJ-Vpy",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b6bb93a0-7da3-4cb5-95cc-6fa2fcd0295e",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1531189360611,
          "user_tz": -330,
          "elapsed": 2835,
          "user": {
            "displayName": "Arth Dh",
            "photoUrl": "//lh5.googleusercontent.com/-k3dUymHUDVw/AAAAAAAAAAI/AAAAAAAANZs/XNCIjM0QmZU/s50-c-k-no/photo.jpg",
            "userId": "110320452319374550954"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mdatalab\u001b[0m/  \u001b[01;34mdrive\u001b[0m/\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "W_UUYoas93Dk",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EI0ldJjnDYrc",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "models_path = 'drive/projects/GOT_LSTM/models'\n",
        "if not os.path.exists(models_path):\n",
        "  os.mkdir(models_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HJ0B_NGXDdJb",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "data_path = 'drive/projects/GOT_LSTM/data'\n",
        "if not os.path.exists(data_path):\n",
        "  os.mkdir(data_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xUqEBs-S93Dn",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "data_path = os.path.join(data_path, '001ssb.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2gNf-Fb693Dq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "text = open(path).read().lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZQ2FFyyY93Ds",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "45432bab-a5b2-4f15-a9da-5d6999598aec",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1531190415760,
          "user_tz": -330,
          "elapsed": 962,
          "user": {
            "displayName": "Arth Dh",
            "photoUrl": "//lh5.googleusercontent.com/-k3dUymHUDVw/AAAAAAAAAAI/AAAAAAAANZs/XNCIjM0QmZU/s50-c-k-no/photo.jpg",
            "userId": "110320452319374550954"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print(len(text))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1607894\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4KjJUhPq93D0",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9d324dcc-540d-4ee7-a25a-0ba7e58f6922",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1531190420860,
          "user_tz": -330,
          "elapsed": 1020,
          "user": {
            "displayName": "Arth Dh",
            "photoUrl": "//lh5.googleusercontent.com/-k3dUymHUDVw/AAAAAAAAAAI/AAAAAAAANZs/XNCIjM0QmZU/s50-c-k-no/photo.jpg",
            "userId": "110320452319374550954"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "max_len = 100\n",
        "step = 8\n",
        "sentences = []\n",
        "next_chars= [] \n",
        "\n",
        "for i in range(0,len(text)-max_len,step):\n",
        "    sentences.append(text[i:i+max_len])\n",
        "    next_chars.append(text[i+max_len])\n",
        "print('number of sequences: ', len(sentences))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of sequences:  200975\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FPw4NXgy93D4",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6f560938-90e4-4f90-d202-4f3cb52957eb",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1531190422409,
          "user_tz": -330,
          "elapsed": 894,
          "user": {
            "displayName": "Arth Dh",
            "photoUrl": "//lh5.googleusercontent.com/-k3dUymHUDVw/AAAAAAAAAAI/AAAAAAAANZs/XNCIjM0QmZU/s50-c-k-no/photo.jpg",
            "userId": "110320452319374550954"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "print('unique: ',len(chars))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unique:  53\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bKYHSvaS93D7",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "chars_indices = dict((char, chars.index(char)) for char in chars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KQBIPy-J93D-",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "x = np.zeros((len(sentences), max_len,len(chars)), dtype = np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype= np.bool)\n",
        "\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i,t, chars_indices[char]] =1\n",
        "    y[i, chars_indices[next_chars[i]]] =1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WYBZBW0i93EA",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from keras import layers\n",
        "from keras.models import load_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gNDIEZPq93EE",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(layers.LSTM(128, input_shape=(max_len, len(chars)),return_sequences= True))\n",
        "model.add(layers.Dropout(0.3))\n",
        "model.add(layers.LSTM(128))\n",
        "model.add(layers.Dropout(0.3))\n",
        "model.add(layers.Dense(len(chars),activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W0LiAJ8F93EG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model.compile(optimizer= keras.optimizers.RMSprop(lr =0.01), loss = 'categorical_crossentropy')\n",
        "\n",
        "saved_model = 'LSTM_GOT.h5'\n",
        "path = os.path.join(models_path, saved_model)\n",
        "model =load_model(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SNiO7y7C93EJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def sample(preds, temperature= 1.0):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds)/ temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1,preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cdMsCXQ293EM",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "import sys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-ZHVRdxW93EP",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 4406
        },
        "outputId": "e8d611f3-f9a2-4cea-986e-4f83444f6726",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1531197683439,
          "user_tz": -330,
          "elapsed": 6582673,
          "user": {
            "displayName": "Arth Dh",
            "photoUrl": "//lh5.googleusercontent.com/-k3dUymHUDVw/AAAAAAAAAAI/AAAAAAAANZs/XNCIjM0QmZU/s50-c-k-no/photo.jpg",
            "userId": "110320452319374550954"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "callbacks = [keras.callbacks.ModelCheckpoint(filepath=path, monitor='loss', save_best_only= True)]\n",
        "temp = 0.5\n",
        "\n",
        "for epoch in range(1,20):\n",
        "    print('\\nepoch', epoch)\n",
        "    history = model.fit(x,y, batch_size =256 , epochs= 1,callbacks= callbacks,validation_split=0.3)\n",
        "    start_index = random.randint(0,len(text)-max_len-1)\n",
        "    generated_text = text[start_index:start_index +max_len]\n",
        "    print('\\n---Generating with seed: \"'+ generated_text + '\"')\n",
        "    print('\\n-----temperature: ', temp,'\\n')\n",
        "    sys.stdout.write(generated_text)\n",
        "    for i in range(100):\n",
        "        sampled = np.zeros((1,max_len, len(chars)))\n",
        "\n",
        "        for t, char in enumerate(generated_text):\n",
        "            sampled[0,t,chars_indices[char]] =1\n",
        "        preds = model.predict(sampled, verbose=0)[0]\n",
        "        next_index = sample(preds, temp)\n",
        "        next_char = chars[next_index]\n",
        "\n",
        "        generated_text += next_char\n",
        "        generated_text = generated_text[1:]\n",
        "\n",
        "        sys.stdout.write(next_char)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 1\n",
            "Train on 140682 samples, validate on 60293 samples\n",
            "Epoch 1/1\n",
            "140682/140682 [==============================] - 309s 2ms/step - loss: 1.7338 - val_loss: 1.6423\n",
            "\n",
            "---Generating with seed: \"ds are like swords, i do fear. the old ones go to rust. ah, and here is our milk.\" the serving girl \"\n",
            "\n",
            "-----temperature:  0.5 \n",
            "\n",
            "ds are like swords, i do fear. the old ones go to rust. ah, and here is our milk.\" the serving girl some of the \n",
            "nothing to the wamy had feet to were on stark said come lord have a nother, "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "stook would\n",
            "epoch 2\n",
            "Train on 140682 samples, validate on 60293 samples\n",
            "Epoch 1/1\n",
            "140682/140682 [==============================] - 309s 2ms/step - loss: 1.6994 - val_loss: 1.6216\n",
            "\n",
            "---Generating with seed: \"ys was your king, was he not?\" \n",
            "\"he was, my lady.\" \n",
            "\"viserys is dead. i am his heir, the last blood \"\n",
            "\n",
            "-----temperature:  0.5 \n",
            "\n",
            "ys was your king, was he not?\" \n",
            "\"he was, my lady.\" \n",
            "\"viserys is dead. i am his heir, the last blood \n",
            "the place in the not on the knight in the keave that was it to"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " her mording a knight was \n",
            "a stell an\n",
            "epoch 3\n",
            "Train on 140682 samples, validate on 60293 samples\n",
            "Epoch 1/1\n",
            "140682/140682 [==============================] - 313s 2ms/step - loss: 1.6756 - val_loss: 1.6062\n",
            "\n",
            "---Generating with seed: \"nd she was knee-deep in foulsmelling water, wishing she could \n",
            "dance upon it as syrio might have, an\"\n",
            "\n",
            "-----temperature:  0.5 \n",
            "\n",
            "nd she was knee-deep in foulsmelling water, wishing she could \n",
            "dance upon it as syrio might have, and"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " their should be standing for her that the hound seemed a moron stark of the mountains and strow a \n",
            "epoch 4\n",
            "Train on 140682 samples, validate on 60293 samples\n",
            "Epoch 1/1\n",
            "102912/140682 [====================>.........] - ETA: 1:15 - loss: 1.6517"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "140682/140682 [==============================] - 313s 2ms/step - loss: 1.6571 - val_loss: 1.6072\n",
            "\n",
            "---Generating with seed: \"oot \n",
            "king.\" he clapped his hands together. \"a cart! bring cart for khal rhaggat! \" \n",
            "five thousand do\"\n",
            "\n",
            "-----temperature:  0.5 \n",
            "\n",
            "oot \n",
            "king.\" he clapped his hands together. \"a cart! bring cart for khal rhaggat! \" \n",
            "five thousand down the came that that.\" \n",
            "\"the barrister with the it to him is a said in the storight their fire woul\n",
            "epoch 5\n",
            "Train on 140682 samples, validate on 60293 samples\n",
            "Epoch 1/1\n",
            " 29440/140682 [=====>........................] - ETA: 3:44 - loss: 1.6370"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "140682/140682 [==============================] - 314s 2ms/step - loss: 1.6419 - val_loss: 1.5913\n",
            "\n",
            "---Generating with seed: \"not avail much against foemen who would not fall because they were already dead; even \n",
            "arms and armo\"\n",
            "\n",
            "-----temperature:  0.5 \n",
            "\n",
            "not avail much against foemen who would not fall because they were already dead; even \n",
            "arms and armor from his hands. she said, what had not the fareched the burmed in the head to have from his forsor\n",
            "epoch 6\n",
            "Train on 140682 samples, validate on 60293 samples\n",
            "Epoch 1/1\n",
            " 20480/140682 [===>..........................] - ETA: 4:02 - loss: 1.5975"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "140682/140682 [==============================] - 314s 2ms/step - loss: 1.6292 - val_loss: 1.5903\n",
            "\n",
            "---Generating with seed: \"w. \n",
            "you always woke up in the instant before you hit the ground. \n",
            "and if you don't? the voice asked.\"\n",
            "\n",
            "-----temperature:  0.5 \n",
            "\n",
            "w. \n",
            "you always woke up in the instant before you hit the ground. \n",
            "and if you don't? the voice asked. \"you think and see the first his knight of the kingsrawn to the time and how the lord more the boot\n",
            "epoch 7\n",
            "Train on 140682 samples, validate on 60293 samples\n",
            "Epoch 1/1\n",
            " 19200/140682 [===>..........................] - ETA: 4:04 - loss: 1.5884"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "140682/140682 [==============================] - 314s 2ms/step - loss: 1.6158 - val_loss: 1.5752\n",
            "\n",
            "---Generating with seed: \"sguard. can he still be a \n",
            "knight?\" \n",
            "\"no,\" ned said. he saw no use in lying to her. \"yet someday he \"\n",
            "\n",
            "-----temperature:  0.5 \n",
            "\n",
            "sguard. can he still be a \n",
            "knight?\" \n",
            "\"no,\" ned said. he saw no use in lying to her. \"yet someday he stark in the cold than the hand. a brone was a grey of the young what had for the throne stark of th\n",
            "epoch 8\n",
            "Train on 140682 samples, validate on 60293 samples\n",
            "Epoch 1/1\n",
            " 19200/140682 [===>..........................] - ETA: 4:02 - loss: 1.6163"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "140682/140682 [==============================] - 315s 2ms/step - loss: 1.6076 - val_loss: 1.5728\n",
            "\n",
            "---Generating with seed: \"raxes, vhaghar. tyrion had stood between their gaping jaws, \n",
            "wordless and awed. you could have ridde\"\n",
            "\n",
            "-----temperature:  0.5 \n",
            "\n",
            "raxes, vhaghar. tyrion had stood between their gaping jaws, \n",
            "wordless and awed. you could have ridden was he said still and the way, the day was bore and the boy. \n",
            "the room the stark should the think \n",
            "epoch 9\n",
            "Train on 140682 samples, validate on 60293 samples\n",
            "Epoch 1/1\n",
            " 19200/140682 [===>..........................] - ETA: 4:06 - loss: 1.5887"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "140682/140682 [==============================] - 314s 2ms/step - loss: 1.6010 - val_loss: 1.5623\n",
            "\n",
            "---Generating with seed: \"e others had gotten you.\" \n",
            "\"it was the grumkins,\" tyrion told him, laughing. jon snow smiled. stark \"\n",
            "\n",
            "-----temperature:  0.5 \n",
            "\n",
            "e others had gotten you.\" \n",
            "\"it was the grumkins,\" tyrion told him, laughing. jon snow smiled. stark morrow to his tart from the stirry to the think the head.\" \n",
            "\"i stared his lest of the ranger of the \n",
            "epoch 10\n",
            "Train on 140682 samples, validate on 60293 samples\n",
            "Epoch 1/1\n",
            " 19200/140682 [===>..........................] - ETA: 4:04 - loss: 1.5896"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "140682/140682 [==============================] - 312s 2ms/step - loss: 1.5892 - val_loss: 1.5607\n",
            "\n",
            "---Generating with seed: \" the eunuch knew too much and did too little. grand maester pycelle seemed \n",
            "more cersei's creature w\"\n",
            "\n",
            "-----temperature:  0.5 \n",
            "\n",
            " the eunuch knew too much and did too little. grand maester pycelle seemed \n",
            "more cersei's creature with the wolf had been stone house on the while beard soon for the drops of the tall and stands and t\n",
            "epoch 11\n",
            "Train on 140682 samples, validate on 60293 samples\n",
            "Epoch 1/1\n",
            " 19200/140682 [===>..........................] - ETA: 4:03 - loss: 1.5720"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "140682/140682 [==============================] - 312s 2ms/step - loss: 1.5804 - val_loss: 1.5606\n",
            "\n",
            "---Generating with seed: \" would bind her and drag her behind his horse all the way up the mother of \n",
            "mountains. she put her a\"\n",
            "\n",
            "-----temperature:  0.5 \n",
            "\n",
            " would bind her and drag her behind his horse all the way up the mother of \n",
            "mountains. she put her acks in the woman said. \"the than the still as not the tringar would stark and the boy some woman sai\n",
            "epoch 12\n",
            "Train on 140682 samples, validate on 60293 samples\n",
            "Epoch 1/1\n",
            " 19200/140682 [===>..........................] - ETA: 4:05 - loss: 1.5552"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "140682/140682 [==============================] - 313s 2ms/step - loss: 1.5741 - val_loss: 1.5673\n",
            "\n",
            "---Generating with seed: \"cream and shout. taunts and obscenities filled \n",
            "the air. sansa had hidden her face in her hands. \n",
            "he\"\n",
            "\n",
            "-----temperature:  0.5 \n",
            "\n",
            "cream and shout. taunts and obscenities filled \n",
            "the air. sansa had hidden her face in her hands. \n",
            "he said. \"i get the stark took to her and \n",
            "wanted in the khaled to with her one enough, the way the co\n",
            "epoch 13\n",
            "Train on 140682 samples, validate on 60293 samples\n",
            "Epoch 1/1\n",
            " 19200/140682 [===>..........................] - ETA: 4:03 - loss: 1.5542"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "140682/140682 [==============================] - 312s 2ms/step - loss: 1.5665 - val_loss: 1.5670\n",
            "\n",
            "---Generating with seed: \"fe, you do not ask a slave, you tell her. she will do as you \n",
            "command.\" he jumped down from the alta\"\n",
            "\n",
            "-----temperature:  0.5 \n",
            "\n",
            "fe, you do not ask a slave, you tell her. she will do as you \n",
            "command.\" he jumped down from the altarment here. \"the wall to mean's thing her not and the septa tompest \n",
            "to looked him. \"why stoned and \n",
            "epoch 14\n",
            "Train on 140682 samples, validate on 60293 samples\n",
            "Epoch 1/1\n",
            " 19200/140682 [===>..........................] - ETA: 4:04 - loss: 1.5434"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "140682/140682 [==============================] - 312s 2ms/step - loss: 1.5575 - val_loss: 1.5434\n",
            "\n",
            "---Generating with seed: \" room they have in the dreadfort, where the boltons hang the skins of their enemies.\" \n",
            "\"that's just \"\n",
            "\n",
            "-----temperature:  0.5 \n",
            "\n",
            " room they have in the dreadfort, where the boltons hang the skins of their enemies.\" \n",
            "\"that's just would be hust every father is when he was hundred man cares of the tree of the rure of the mord with\n",
            "epoch 15\n",
            "Train on 140682 samples, validate on 60293 samples\n",
            "Epoch 1/1\n",
            " 19200/140682 [===>..........................] - ETA: 4:03 - loss: 1.5450"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "140682/140682 [==============================] - 313s 2ms/step - loss: 1.5521 - val_loss: 1.5394\n",
            "\n",
            "---Generating with seed: \"ed with a two-handed \n",
            "greatsword. \n",
            "ser rodrik shouted \"winterfell!\" and rode to meet him, with bronn\"\n",
            "\n",
            "-----temperature:  0.5 \n",
            "\n",
            "ed with a two-handed \n",
            "greatsword. \n",
            "ser rodrik shouted \"winterfell!\" and rode to meet him, with bronn stropped at the lord of the back before the servand stark of the creat called and she was no be the\n",
            "epoch 16\n",
            "Train on 140682 samples, validate on 60293 samples\n",
            "Epoch 1/1\n",
            " 19200/140682 [===>..........................] - ETA: 4:03 - loss: 1.5384"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "140682/140682 [==============================] - 312s 2ms/step - loss: 1.5427 - val_loss: 1.5365\n",
            "\n",
            "---Generating with seed: \"\n",
            "fostering the boy elsewhere would be a grievous affront to him.\" \n",
            "\"i have more concern for my nephe\"\n",
            "\n",
            "-----temperature:  0.5 \n",
            "\n",
            "\n",
            "fostering the boy elsewhere would be a grievous affront to him.\" \n",
            "\"i have more concern for my nephe and upforlar said with the ser jast stalled her from the men had cool that of the cright to the lor\n",
            "epoch 17\n",
            "Train on 140682 samples, validate on 60293 samples\n",
            "Epoch 1/1\n",
            " 19200/140682 [===>..........................] - ETA: 4:03 - loss: 1.5286"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "140682/140682 [==============================] - 313s 2ms/step - loss: 1.5390 - val_loss: 1.5437\n",
            "\n",
            "---Generating with seed: \"ded to trick. \"how many guards does my father have?\" she asked him as they descended to her \n",
            "bedcham\"\n",
            "\n",
            "-----temperature:  0.5 \n",
            "\n",
            "ded to trick. \"how many guards does my father have?\" she asked him as they descended to her \n",
            "bedchame the took to the lade, when the store to the count with his true and dod can the start, his brothe"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "r\n",
            "epoch 18\n",
            "Train on 140682 samples, validate on 60293 samples\n",
            "Epoch 1/1\n",
            "140682/140682 [==============================] - 313s 2ms/step - loss: 1.5340 - val_loss: 1.5321\n",
            "\n",
            "---Generating with seed: \"daughters of hers, dressed in men's mail. maege is a hoary old \n",
            "snark, stubborn, short-tempered, and\"\n",
            "\n",
            "-----temperature:  0.5 \n",
            "\n",
            "daughters of hers, dressed in men's mail. maege is a hoary old \n",
            "snark, stubborn, short-tempered, and ser brad and hamed him. \"bran is hand was was gone of \n",
            "hear.\" \n",
            "\"the man did not well"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " her lannisters\n",
            "epoch 19\n",
            "Train on 140682 samples, validate on 60293 samples\n",
            "Epoch 1/1\n",
            "140682/140682 [==============================] - 312s 2ms/step - loss: 1.5328 - val_loss: 1.5523\n",
            "\n",
            "---Generating with seed: \" oversize head, they found \n",
            "a huge bucket-shaped greathelm topped with a foot-long triangular spike.\"\n",
            "\n",
            "-----temperature:  0.5 \n",
            "\n",
            " oversize head, they found \n",
            "a huge bucket-shaped greathelm topped with a foot-long triangular spike. \n",
            "\"no bare stead with the hall \n",
            "gareing of the chil"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "dren said. some still \n",
            "with of the mord said. \n",
            "\"y"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OKtbjnIx93ES",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "88aabeee-9914-4f2f-b5e4-841587ea4db1",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1531200064352,
          "user_tz": -330,
          "elapsed": 51137,
          "user": {
            "displayName": "Arth Dh",
            "photoUrl": "//lh5.googleusercontent.com/-k3dUymHUDVw/AAAAAAAAAAI/AAAAAAAANZs/XNCIjM0QmZU/s50-c-k-no/photo.jpg",
            "userId": "110320452319374550954"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "temp =0.475\n",
        "generated_text = text[200:300]\n",
        "print('\\n---Generating with seed: \"'+ generated_text + '\"\\n')\n",
        "for i in range(400):\n",
        "    sampled = np.zeros((1,max_len, len(chars)))\n",
        "    for t, char in enumerate(generated_text):\n",
        "        sampled[0,t,chars_indices[char]] =1\n",
        "    preds = model.predict(sampled, verbose=0)[0]\n",
        "        \n",
        "    next_index = sample(preds, temp)\n",
        "    next_char = chars[next_index]\n",
        "\n",
        "    generated_text += next_char\n",
        "    generated_text = generated_text[1:]\n",
        "    sys.stdout.write(next_char)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "---Generating with seed: \"o the dead frighten you?\" ser waymar royce asked with just the hint of a smile. \n",
            "gared did not rise \"\n",
            "\n",
            "she would"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " get the stone come to the torch. \n",
            "\"it were the blade still do a can shad the mountace, but we see you warmers and make any was it was had and lord beside the store she had go came and for me. he had the hands the porlast of the lord and fat or the warve and foress to the king was a head to \n",
            "the would and some and she took the bran was stark great of catelyn grown the shadows in his think"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HfGYT-vS93EV",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}